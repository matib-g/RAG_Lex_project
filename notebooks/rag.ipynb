{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8aeefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_cpp import Llama\n",
    "import chromadb\n",
    "import json\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"/Users/mateuszbulanda-gorol/Desktop/Projects/rag_lex_project/vectordb/chroma_db\"\n",
    "COLLECTION_NAME = \"isap_acts\"\n",
    "EMB_MODEL = \"clarin-pl/roberta-polish-sentence-transformer-v1\"\n",
    "LLAMA_GGUF_PATH = \"/Users/mateuszbulanda-gorol/Desktop/Projects/rag_lex_project/models\"\n",
    "LLAMA_N_CTX = 4096\n",
    "TOP_K = 5\n",
    "MAX_GEN_TOKENS = 512\n",
    "PROMPT_MAX_CHARS = 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embedding_model(model_name: str):\n",
    "    print(f\"[init] Loading embedding model: {model_name}\")\n",
    "    return SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_chroma_client(chroma_path: str):\n",
    "    print(f\"[init] Opening CromaDB persistent as: {chroma_path}\")\n",
    "    client = chromadb.PersistentClient(path=chroma_path)\n",
    "    collection = client.get_collection(COLLECTION_NAME)\n",
    "    return client, collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2638a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llama(model_path: str, n_ctx: int):\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"LLAMA model file not found at: {model_path}\")\n",
    "    print(f\"[init] Loading model (gguf) from {model_path}\")\n",
    "    llm = Llama(model_path=model_path, n_ctx=n_ctx, n_threads=8, n_gpu_layers=-1)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_retrieval_centext(collection, emb_model, query: str, top_k: int, char_limit: int = PROMPT_MAX_CHARS):\n",
    "    \"\"\"\n",
    "    1. Embed query\n",
    "    2. Query Chroma\n",
    "    3. Compose contect string with citations\n",
    "    Returns: contect_text, hits (list of dicts)\n",
    "    \"\"\"\n",
    "    print(\"[retrieve] Embedding query...\")\n",
    "    q_emb = emb_model.encode([query]).tolist()\n",
    "    print(\"[retrive] Querying ChromaDB...\")\n",
    "    results = collection.query(query_embeddings=q_emb, n_results=top_k, include=[\"documents\",\"metadatas\",\"distances\"])\n",
    "    docs = results[\"documents\"][0]\n",
    "    metadatas = results[\"metadatasa\"][0]\n",
    "    distances = results.get(\"distances\", [[]])[0]\n",
    "\n",
    "    hits = []\n",
    "    for i, (doc, meta, dist) in enumerate(zip(docs, metadatas, distances)):\n",
    "        # Normalize metadata representation for citation\n",
    "        citation = meta.get(\"filename\", \"unknown\")\n",
    "        if meta.get(\"year\") and meta.get(\"pos\"):\n",
    "            citation = f\"{meta.get('publisher', '')}_{meta.get('year', '')}_poz.{meta.get('pos', '')}\"\n",
    "        # capture chunk id if present\n",
    "        chunk_id = meta.get(\"chunk_id\", None)\n",
    "        hits.append({\n",
    "            \"rank\": i+1\n",
    "            ,\"score\": float(dist) if dist is not None else None\n",
    "            ,\"text\": doc\n",
    "            ,\"meta\": meta\n",
    "            ,\"citation\": f\"{citation}#{chunk_id}\" if chunk_id is not None else citation\n",
    "        })\n",
    "    \n",
    "    # Compose context text with simple formatting\n",
    "    parts = []\n",
    "    total_chars = 0\n",
    "    for h in hits:\n",
    "        block = f\"[Źródło: {h['citation']}]\\n{h['text'].strip()}\\n\"\n",
    "        if total_chars + len(block) > char_limit:\n",
    "            break\n",
    "        parts.append(block)\n",
    "        total_chars += len(block)\n",
    "    \n",
    "    context_text = \"\\n---\\n\".join(parts)\n",
    "    return context_text, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ea72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Polish prompt template. Instruct model to answer based only on provided sources and cite them.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "        Jesteś ekspertem prawa polskiego. Odpowiedz na zadane pytanie **wyłącznie** na podstawie poniszych fragmentów ustaw (nie wymyślaj dodatkowych informacji).\n",
    "        Jeśli fragmenty są sprzeczne lub niewystarczające, zaznacz to i wskaż jakie dodatkowe źródła byłyby potrzebne.\n",
    "        Podaj odpowiedź po polsku. Na końcu krótkie odniesienie do uytych źródeł (np. \"Źródła: DU_2020_poz.123#4).\n",
    "\n",
    "        Poniej fragmenty aktów:\n",
    "        {context}\n",
    "\n",
    "        Pytanie: {question}\n",
    "\n",
    "        Odpowiedź:\n",
    "        \"\"\".strip()\n",
    "    # optionally shorten whitespace\n",
    "    return textwrap.shorten(prompt, width=LLAMA_N_CTX*4, placeholder=\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2eaf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(llm, prompt: str, max_tokens: int = MAX_GEN_TOKENS):\n",
    "    print(\"[gen] Generating answer from LLM...\")\n",
    "    resp = llm.create(prompt=prompt, max_tokens=max_tokens, temperature=0.0)\n",
    "    text = resp.get(\"choices\", [{}])[0].get(\"text\", \"\").strip()\n",
    "    return text, resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f68ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_hits(hits: List[dict]):\n",
    "    print(\"\\n[Retrived passages]\")\n",
    "    for h in hits:\n",
    "        print(f\"Rank{h['rank']} | citation: {h['citation']} | score: {h['score']}\")\n",
    "        snippet = h[\"text\"][:400].replace(\"\\n\", \" \")\n",
    "        print(\" \",snippet, \"...\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78fe162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    emb_model = init_embedding_model(EMB_MODEL)\n",
    "    client, collection = init_chroma_client(CHROMA_PATH)\n",
    "    llm = init_llama(LLAMA_GGUF_PATH, n_ctx=LLAMA_N_CTX)\n",
    "\n",
    "    query = args.query\n",
    "    print(f\"\\n[query] {query}\\n\")\n",
    "\n",
    "    context_text, hits = build_retrieval_centext(collection, emb_model, query, top_k=TOP_K)\n",
    "    if not context_text.strip():\n",
    "        print(\"[warn] Brak pasujących fragmentów w bazie.\")\n",
    "    else:\n",
    "        pretty_print_hits(hits)\n",
    "    \n",
    "    prompt = build_prompt(query, context_text)\n",
    "    # optional: print prompt preview (truncated)\n",
    "    print(\"\\n[prompt preview]\\n\", prompt[:2000], \"\\n...\")\n",
    "\n",
    "    answer, raw = generate_answer(llm, prompt, max_tokens=MAX_GEN_TOKENS)\n",
    "\n",
    "    print(\"\\n[FINAL ANSWER]\\n\")\n",
    "    print(answer)\n",
    "    print(\"\\n[RAW LLM RESPONSE METADATA]\\n\", json.dump(raw, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c1bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Simple RAG: Chroma + PLLuM(gguf via llama-cpp)\")\n",
    "parser.add_argument(\"--query\", \"-q\", type=str, required=True, help=\"Pytanie po poslku\")\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f314bab",
   "metadata": {},
   "source": [
    "python rag_llama_chroma.py -q \"Jakie są zasady wprowadzenia stanu wyjątkowego w Polsce?\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_lex_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
